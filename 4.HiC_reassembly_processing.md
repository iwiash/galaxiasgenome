## Initial data processing

```
module load SAMtools
module load pigz
module load NanoComp
module load cutadapt
```

### 1. convert to fastq
```
samtools fastq -@4 GbrevPB.hifi_reads.bam | pigz > gbrev_hifi_reads.fq.gz
```
```
#Output:
[M::bam2fq_mainloop] discarded 0 singletons
[M::bam2fq_mainloop] processed 6826044 reads
```
### 2. NanoComp
```
NanoComp --fastq gbrev_hifi_reads.fq.gz --names Gbrev_PacBio_HiFi --outdir nanocomp_assembly_hifi
```
### 3. cutadapt
```
cutadapt -b "AAAAAAAAAAAAAAAAAATTAACGGAGGAGGAGGA;min_overlap=35" \
-b "ATCTCTCTCTTTTCCTCCTCCTCCGTTGTTGTTGTTGAGAGAGAT;min_overlap=45" \
--discard-trimmed \
-o /dev/null \
gbrev_hifi_reads.fq.gz \
-j 0 \
--revcomp \
-e 0.05
```
```
=== Summary ===

Total reads processed:               6,826,044
Reads with adapters:                         1 (0.0%)
Reverse-complemented:                        0 (0.0%)

== Read fate breakdown ==
Reads discarded as trimmed:                  1 (0.0%)
Reads written (passing filters):     6,826,043 (100.0%)

Total basepairs processed: 91,974,606,973 bp
Total written (filtered):  91,974,589,506 bp (100.0%)

=== Adapter 1 ===

Sequence: AAAAAAAAAAAAAAAAAATTAACGGAGGAGGAGGA; Type: variable 5'/3'; Length: 35; Trimmed: 0 times; Reverse-complemented: 0 times

=== Adapter 2 ===

Sequence: ATCTCTCTCTTTTCCTCCTCCTCCGTTGTTGTTGTTGAGAGAGAT; Type: variable 5'/3'; Length: 45; Trimmed: 1 times; Reverse-complemented: 0 times
0 times, it overlapped the 5' end of a read
1 times, it overlapped the 3' end or was within the read

Minimum overlap: 45
No. of allowed errors:
1-19 bp: 0; 20-39 bp: 1; 40-45 bp: 2

Overview of removed sequences (5')
length  count   expect  max.err error counts



Overview of removed sequences (3' or within)
length  count   expect  max.err error counts
17444   1       0.0     2       1
```

### 4. trim UMIS off HiC
```
zcat Bruce_fish_HiC_S1_R1_001.fastq.gz | awk '{ if(NR%2==0) {print substr($1,10)} else {print} }' | gzip > Fish_HiC_trimmed_R1_001.fastq.gz

zcat Bruce_fish_HiC_S1_R2_001.fastq.gz | awk '{ if(NR%2==0) {print substr($1,10)} else {print} }' | gzip > Fish_HiC_trimmed_R2_001.fastq.gz
```

## Hifiasm assembly
### 1. hifiasm assembly and conversion to fasta
```
#!/bin/bash -e
#SBATCH --job-name=hic_asm_rerun # job name (shows up in the queue)
#SBATCH --time=48:00:00      # Walltime (HH:MM:SS) 48h
#SBATCH --mem=230G          # Memory in MB
#SBATCH --account=uoo02831
#SBATCH --cpus-per-task=128
#SBATCH --partition=milan

module load hifiasm

hifiasm \
    -o GB_full_HIC \
    -t128 \
    --h1 Fish_HiC_trimmed_R1_001.fastq.gz \
    --h2 Fish_HiC_trimmed_R2_001.fastq.gz \
    gbrev_hifi_reads.fq.gz\
    2> test.log

## h1 and h2 are Hi-C forward and reverse reads

echo "assembly done, starting conversion"

## converts gfa file to final fasta assembly

awk '/^S/{print ">"$2;print $3}' \
    GB_full_HIC.bp.p_ctg.gfa \
    > GB_full_HIC.p_ctg.fa

## p_ctg is the primary unphased contigs - could use haplotypes 1 and 2 if want a phased asm
echo "conversion complete"

```

### 2. gfastats
```
module load gfastats

gfastats GB_full_HIC.p_ctg.fa
```
```
+++Assembly summary+++: 
# scaffolds: 664
Total scaffold length: 630588521
Average scaffold length: 949681.51
Scaffold N50: 7215088
Scaffold auN: 8253254.37
Scaffold L50: 27
Largest scaffold: 28003021
Smallest scaffold: 12723
# contigs: 664
Total contig length: 630588521
Average contig length: 949681.51
Contig N50: 7215088
Contig auN: 8253254.37
Contig L50: 27
Largest contig: 28003021
Smallest contig: 12723
# gaps in scaffolds: 0
Total gap length in scaffolds: 0
Average gap length in scaffolds: 0.00
Gap N50 in scaffolds: 0
Gap auN in scaffolds: 0.00
Gap L50 in scaffolds: 0
Largest gap in scaffolds: 0
Smallest gap in scaffolds: 0
Base composition (A:C:G:T): 176460736:138981680:138809043:176337062
GC content %: 44.05
# soft-masked bases: 0
# segments: 664
Total segment length: 630588521
Average segment length: 949681.51
# gaps: 0
# paths: 664
```
slightly smaller/slightly more contigs than non - HiC assembly

## Quality control
### 1. meryl
```
#!/bin/bash -e
#SBATCH --account=uoo02831
#SBATCH --job-name=meryl
#SBATCH --time=02:00:00
#SBATCH --cpus-per-task=16
#SBATCH --mem=64G
#SBATCH --partition=milan

# Modules
module purge
module load Merqury/1.3-Miniconda3

# Run
meryl count k=17 memory=64 threads=16 \
  gbrev_hifi_reads.fq.gz \
  output gb-hic-read-kmerdb.meryl
```
create hist - put in genomescope2
```
meryl histogram gb-hic-read-kmerdb.meryl > gb-hic-read-kmerdb.hist
```
http://genomescope.org/genomescope2.0/analysis.php?code=MBW0H4GmA4NoDrPLe755 #17mer (lowkey bad)

http://genomescope.org/genomescope2.0/analysis.php?code=hJkftalqzUQm2OGmWHIr #25mer (better)

http://genomescope.org/genomescope2.0/analysis.php?code=ech9pLIBECiwO7EhVusf #30mer (kind of best weirdly enough)

### 2. fastplong
gitclone fastplong into conda env - lowkey idk if putting it in a conda env is necessary.. idk anything about all that. but this is what i did so
```
conda create -n fastplong
conda activate fastplong

conda install conda-forge::libdeflate
conda install conda-forge::isa-l
conda install conda-forge::libhwy

git clone https://github.com/OpenGene/fastplong.git
```
run
```
fastplong -i gbrev_hifi_reads.fq.gz -o cleaned_gb_hifi_reads.fq
```
output says it didnt find any more adapters so take original reads file forward
also idk if fq is the right extension but since im not actually using it it doesnt matter anyway

### 3. Minimap2

Rerun minimap2 on HiC primary assembly

```
#!/bin/bash -e
#SBATCH --job-name=gbminimaphic # Job name (shows up in the queue)
#SBATCH --time=20:00:00      # Walltime (HH:MM:SS) 48h
#SBATCH --mem=32G            # Memory in MB
#SBATCH --account=uoo02831
#SBATCH --cpus-per-task=16

module load minimap2/2.24-GCC-11.3.0
module load SAMtools/1.16.1-GCC-11.3.0

minimap2 \
    -t 16 -ax map-hifi \
    GB_full_HIC.p_ctg.fa gbrev_hifi_reads.fq.gz \
    --secondary=no | samtools sort -m 5G -o hic_asm_minimap_align.bam -T temp.ali \
    2> minimap2_hic_errors.log

echo "alignment and file conversion complete"

```

this creates a sam out[it (-a flag) which i believe i needed for purge_haplotigs but need a paf file for purgedups oops
```
#!/bin/bash -e
#SBATCH --job-name=gbminimaphic # Job name (shows up in the queue)
#SBATCH --time=15:00:00      # Walltime (HH:MM:SS) 48h
#SBATCH --mem=32G            # Memory in MB
#SBATCH --account=uoo02831
#SBATCH --cpus-per-task=32

module load minimap2
module load pigz

minimap2 \
	-t 32 -x map-hifi \
	GB_full_HIC.p_ctg.fa gbrev_hifi_reads.fq.gz \
	> hic_minimap_align.paf \
	2> minimap_paf_errors.log

echo "alignment complete"

pigz -p 32 -c hic_minimap_align.paf > hic_minimap_align.paf.gz

echo "compression complete"

```


## 4. Purge_dups

```
module load purge_dups
module load minimap2
```

```
pbcstat hic_minimap_align.paf

calcuts PB.stat > cutoffs 2>calcults.log
```
produces PB.base.cov and PB.stat files

split assembly and do self-self alignment
```
split_fa GB_full.p_ctg.fa > GB_asm.split

minimap2 -xasm5 -DP gb_hic_asm.split gb_hic_asm.split | gzip -c - > gb_hic_asm.split.self.paf.gz
```

identify haplotigs and duplicates - places coordinates of dup/hap regions into file dup_sequences.bed
```
purge_dups -2 -T cutoffs -c PB.base.cov gb_hic_asm.split.self.paf.gz > dup_sequences.bed 2> purge_dups.log
```
purge duplicates from full genome using the bed file above
```
get_seqs -e -p gb_hic_purged dup_sequences.bed GB_full_HIC.p_ctg.fa
```
-e = rm only dups at the end of contigs - remove for more stringent but might cut out actual stuff
produces purged.fa and hap.fa (with prefix) - **gb_hic_purged.fa is final purged draft taken forwards**
```
[W::get_seqs_core] ptg000001l 3240733 3273834 is skipped
[W::get_seqs_core] ptg000007l 302571 1899292 is skipped
[W::get_seqs_core] ptg000009l 3740010 3829306 is skipped
[W::get_seqs_core] ptg000037l 656985 796933 is skipped
[W::get_seqs_core] ptg000038l 3478936 3545999 is skipped
[W::get_seqs_core] ptg000100l 1293521 1362279 is skipped
[W::get_seqs_core] ptg000192l 202589 420872 is skipped
```
### check gfastats of purged assembly:
```
#gb_hic_purged.fa
+++Assembly summary+++: 
# scaffolds: 278
Total scaffold length: 604930116
Average scaffold length: 2176007.61
Scaffold N50: 7780314
Scaffold auN: 8577137.58
Scaffold L50: 25
Largest scaffold: 28003021
Smallest scaffold: 17764
# contigs: 278
Total contig length: 604930116
Average contig length: 2176007.61
Contig N50: 7780314
Contig auN: 8577137.58
Contig L50: 25
Largest contig: 28003021
Smallest contig: 17764
# gaps in scaffolds: 0
Total gap length in scaffolds: 0
Average gap length in scaffolds: 0.00
Gap N50 in scaffolds: 0
Gap auN in scaffolds: 0.00
Gap L50 in scaffolds: 0
Largest gap in scaffolds: 0
Smallest gap in scaffolds: 0
Base composition (A:C:G:T): 170177384:132534868:132376761:169841103
GC content %: 43.79
# soft-masked bases: 0
# segments: 278
Total segment length: 604930116
Average segment length: 2176007.61
# gaps: 0
# paths: 278
```
way less contigs in the purged asm, also smaller - looks good

## Longstitch

```
longstitch run draft=gb_hic_purged reads=gbrev_hifi_reads t=8 out_prefix=gb_hic_fulldraft G=6e8 longmap=hifi
```
check this over but this is my guess of the parameters i need.  not sure if i should round the G= or not
This is taking ages - if doesnt run today run as  job tmrw

maybe also run the ark one? idk what that is but i do have a kmer database from meryl? idk
```
longstitch tigmint-ntLink-arks draft=gb_hic_purged reads=gbrev_hifi_reads kmer_db=(meryl kmer database) t=8 out_prefix=gb_hic_fulldraft_arks G=3.02465058e8 longmap=hifi
```

```
#!/bin/bash -e
#SBATCH --job-name=gblongstitch_nosym # Job name (shows up in the queue)
#SBATCH --time=10:00:00      # Walltime (HH:MM:SS) 48h
#SBATCH --mem=64G            # Memory in MB
#SBATCH --account=uoo02831
#SBATCH --cpus-per-task=8

module purge

module load LongStitch	

longstitch run \
	draft=gb_hic_purged reads=gbrev_hifi_reads \
	t=8 \
	out_prefix=gb_hic_fulldraft_600 \
	span=5 \ #or 10
	dist=1000 \ #or lower
	G=6e8 \
	longmap=hifi \
	2> longmap_err_nosym_600.log

echo "run complete"

```
** IMPORTANT: when running longstitch run on the actual data!!! not sym links as it runs into a memory corruption issue. **
Ran with 6GB of mem so can size down majorly

im unsure which file is the output for this but surely its the fulldraft one?

```
gfastats gb_hic_fulldraft_600.scaffolds.fa
```
```
+++Assembly summary+++: 
# scaffolds: 7421
Total scaffold length: 608380570
Average scaffold length: 81980.94
Scaffold N50: 8948147
Scaffold auN: 8877196.52
Scaffold L50: 23
Largest scaffold: 23509802
Smallest scaffold: 1
# contigs: 13648
Total contig length: 604821348
Average contig length: 44315.75
Contig N50: 548521
Contig auN: 1444694.68
Contig L50: 222
Largest contig: 8111915
Smallest contig: 1
# gaps in scaffolds: 6227
Total gap length in scaffolds: 3559222
Average gap length in scaffolds: 571.58
Gap N50 in scaffolds: 1603
Gap auN in scaffolds: 2410.05
Gap L50 in scaffolds: 643
Largest gap in scaffolds: 13308
Smallest gap in scaffolds: 20
Base composition (A:C:G:T): 169835862:132408726:132449641:170127119
GC content %: 43.79
# soft-masked bases: 94
# segments: 13648
Total segment length: 604821348
Average segment length: 44315.75
# gaps: 6227
# paths: 7421
```
this one is better than the last one was but is still way less contiguous than the input - also the contig N50 is way low but the scaffold N50 is higher.  Not sure what that means but idk doesnt seem ideal
## decided to call it at the purged genome before longstitch

## Repeatmodeler
create database
```
BuildDatabase -name gbrevDB gb_hic_purged.fa
```
```
Building database gbrevDB:
  Reading gb_hic_purged.fa...
Number of sequences (bp) added to database: 278 ( 604930116 bp )
```

run repeatmodeler
```
#!/bin/bash -e
#SBATCH --job-name=gblongstitch_nosym # Job name (shows up in the queue)
#SBATCH --time=72:00:00      # Walltime (HH:MM:SS) 48h
#SBATCH --mem=64G            # Memory in MB
#SBATCH --account=uoo02831
#SBATCH --cpus-per-task=32

module purge

module load RepeatModeler

RepeatModeler \
	-database gbrevDB \
	-pa 8 \
	-LTRStruct \
	>& repeatmodeler_run.out
```
-pa is how many jobs to run in parallel - default RMBlast runs using 4 cores each so 16/4 = 4 ...... i think

-LTRStruct runs the LTR structural discovery pipeline - idk if i need this but its in the example run so. whatever

timed out - up the time and the cores

keeps timing out add flag -recoverDir [ResultDir] option allows you to specify a diretory ( i.e RM_./ ) where a previous run of RepeatModeler was working and it will automatically determine how to continue the analysis.

rm is being a little bitch and wont run past run 6 for some reason and then it just fails so im taking the files out and running them thru repeatclassifier by hand
```
RepeatClassifier \
> -consensi consensi.fa \
> -stockholm families.stk
```
ran as a job but cant figure out if i can parallelise it so decided it might not be worth it 

ran in terminal and it appears to have finished but idk what the output files are supposed to be so maybe it didnt finish properly?  but there was no error or anything
but there is still a temp file in the dir

maybe rerun with a >2 or something to catch the errors or see if there is a verbose option

**Ludo said try running repeatmasker and see what happens so Ive put a run on with my custon library and one with the actinopterygii species option which should hopefully use the zebrafish dfam libraries?
```
#!/bin/bash -e
#SBATCH --job-name=gbrepeatmasker1 # Job name (shows up in the queue)
#SBATCH --time=15:00:00      # Walltime (HH:MM:SS) 48h
#SBATCH --mem=64G            # Memory in MB
#SBATCH --account=uoo02831
#SBATCH --cpus-per-task=36

module purge

module load RepeatMasker


RepeatMasker \
	-pa 36 \
	-gff \
	-lib consensi.fa.classified \
	-dir customMaskerOutput \
	gb_hic_purged.fa

echo "repeatmasking finished"

```

```
#!/bin/bash -e
#SBATCH --job-name=gbrepeatmasker2 # Job name (shows up in the queue)
#SBATCH --time=15:00:00      # Walltime (HH:MM:SS) 48h
#SBATCH --mem=64G            # Memory in MB
#SBATCH --account=uoo02831
#SBATCH --cpus-per-task=36

module purge

module load RepeatMasker


RepeatMasker \
	-pa 36 \
	-gff \
	-species actinopterygii \
	-dir dfamMaskerOutput \
	gb_hic_purged.fa

echo "repeatmasking finished"
```

somewhat nervous abt this gff flag not sure wht it is but i think it makes it output something as a gff file - hopefully in addition to regular outputs but if not im ok just rerunning it as long as it works initially

stats:
```
+++Assembly summary+++: 
# scaffolds: 278
Total scaffold length: 604930116
Average scaffold length: 2176007.61
Scaffold N50: 7780314
Scaffold auN: 8577137.58
Scaffold L50: 25
Largest scaffold: 28003021
Smallest scaffold: 17764
# contigs: 816137
Total contig length: 432442731
Average contig length: 529.87
Contig N50: 1396
Contig auN: 2398.63
Contig L50: 80339
Largest contig: 218811
Smallest contig: 1
# gaps in scaffolds: 815924
Total gap length in scaffolds: 172487385
Average gap length in scaffolds: 211.40
Gap N50 in scaffolds: 654
Gap auN in scaffolds: 29093.18
Gap L50 in scaffolds: 46304
Largest gap in scaffolds: 798589
Smallest gap in scaffolds: 6
Base composition (A:C:G:T): 121407306:94983744:94817369:121234312
GC content %: 43.89
# soft-masked bases: 0
# segments: 816137
Total segment length: 432442731
Average segment length: 529.87
# gaps: 815924
# paths: 278
```

### Ordering by length
index file w samtools
```
samtools faidx gb_hic_purged.fa.masked
```

extract the scaffold names and lengths
```
cut -f 1,2 gb_hic_purged.fa.masked.fai > genome_scaffold_lengths.txt
```

sort by second col (length) in descending order
```
sort -k2,2nr genome_scaffold_lengths.txt > sorted_scaffold_lengths.txt
```

print new names in their own col - by row # and with 0 padding them
```
awk '{ printf("scaffold_%04d\t%s\t%s\n", NR, $1, $2) }' sorted_scaffold_lengths.txt > final_sorted_scaffolds.txt

awk '{print $2, $1}' final_sorted_scaffolds.txt > rename_map.txt
```

did something in here to rename them in the fasta
```
awk \
	'BEGIN { while ((getline < "rename_map.txt") > 0) { split($0, arr, " "); map[arr[1]] = arr[2]; } } \
	{ if ($0 ~ /^>/) { scaffold_name = substr($0, 2); \
	if (scaffold_name in map) { $0 = ">" map[scaffold_name]; } } print $0; }' \
	gb_hic_purged.fa.masked > ordered_gb_hic_purged_masked.fa
```
begin block reads our mapping file which is separated by a " " for all the lines (> 0) and splits them into arr1 and 2

if bit removes the > from the fasta header then checks for it in the map file (substring remove the first chara (>) if it has a >
then replaces with the new name.
print $0 prints the current line - so prints the sequence lines after the headers too

then to reorder them by number
```
seqkit fx2tab --length ordered_gb_hic_purged_masked.fa | sort -t $'\t' -k4,4nr | seqkit tab2fx > reordered_gb_hic_purged_masked.fa
```

now rename them in the vcf file too:
```
awk 'BEGIN { while ((getline < "rename_map.txt") > 0) { split($0, arr, " "); map[arr[1]] = arr[2]; } } \
	{ if ($1 in map) { $1 = map[$1]; } print $0; }' \
	no_island_rerun_bayescan_input.recode.vcf > test_renamed_output.vcf
```
i think doing this changed the column delimiers to spaces so i changed them back with sed
```
sed 's/ /\t/g' test_renamed_output.vcf > fixed_vcf_file.vcf
```

this works but now the header and the contigs dont match which i forsee being a problem.
i can split the header and rename them that way but i cant figure out how to put it back on:
```
awk '/^##/ {print > "vcf_header.txt"}' test_renamed_output.vcf

awk 'BEGIN { while ((getline < "rename_map.txt") > 0) { split($0, arr, " "); map[arr[1]] = arr[2]; } } 
    /##contig=<ID=/ { 
        for (key in map) {
            if ($0 ~ key) { 
                gsub(key, map[key]); 
            }
        }
    } 
    { print $0; }' vcf_header.txt > renamed_vcf_header.txt
```
this makes the header renamed but how do i put it back on the orig vcf........
